#summary Getting the most out of cppao.

= Summary =
The basic object types (`active::object` and `active::shared<>`) are the default types which should offer reasonable results.  Cppao offers other object types with slightly different characteristics.

Cppao actually implements a policy framework, where different aspects can be combined. It is possible to implement new strategies (e.g. to prioritise messages, or to limit the message queue size) in an extensible manner.

= Object types =

* Beware: Here be dragons *

The basic object type is `active::object`, with `active::shared<>` for use with `std::shared_ptr<>`. These are designed to offer the best and safest experience. The implementation details are deliberately hidden because it allows the underlying implementation to change and innovate.

It is possible to select different implementations if you want to try to improve performance, perhaps at the expense of a little safety. Like much multi-threaded code, performance is not always predictable.

See [http://code.google.com/p/cppao/source/browse/trunk/samples/object_types.cpp object_types.cpp] for examples of the different object types.

Available object types:

  * `class active::object` - The basic AO offering a good default. It is not completely safe because the standard C++ scoping rules apply, and attempting to destroy the object during message processing will result in defined behaviour.
  * `class active::fast` - Implements "work stealing" strategy so that a message call can actually execute the message handler if the object is currently idle. This object is safe, but the caller can become blocked. If you have an insane number of active objects, it's possible to run out of stack space.
  * `class active::shared<>` - Same as `active::object` but for use in smart pointers, and is guaranteed to not be destroyed during message processing. 
  * `class active::thread` - Dedicates an OS thread to this object. This object is safe, but can fail due to OS limits on the number of threads.
  * `class active::shared_thread<>` - Dedicates an OS thread to this object, for used in shared objects.
  * `class active::mutexed` - A very simple mutexed object, which does not use queueing. This is safe but a reentrant call could deadlock the object, and the caller could get blocked waiting for the message to process.
  * `class active::try_lock` - Same as `active::mutexed`, however will detect re-entrant calls and fail with an exception.
  * `class active::direct` - Non-active object; message calls are dispatched immediately. This is most definitely not in any way safe, but it's also fast. Beware of stack overflows as well.

= Policy framework =

Cppao actually implements a policy framework which allows different types of AO to be created, and even supports the creation of new implementation strategies by the developer.

The underlying AO is implemented by the `active::object_impl<>` template which needs to be supplied with three different strategy-types (using a Strategy pattern supplied as template parameters). The built-in AO types are just typedefs of `active::object_impl<>`.

{{{
namespace active
{
    template<typename Schedule, typename Queueing, typename Sharing>
    struct object_impl;
}
}}}

== Schedule implementations ==
The schedule determines which AO  is next run.  The provided schedulers are:

  * `active::schedule::thread_pool` (default) - schedules the AO using a thread-pool. This is ideal if the number of AOs is large.
  * `active::schedule::own_thread` - uses a dedicated OS thread to execute the AO. Good if you need pre-emptive multitasking and the number of AOs is limited (<1000 say).
  * `active::schedule::none` - used for those queueing implementations which don't need their own scheduler.

An AO becomes "activated" when it receives its first message. The `thread_pool` scheduler passes the AO to `active::pool`, which puts the AO onto a FIFO of activated objects. A thread (or pool of threads) can then pop the next AO from the queue, and run the AO (using `active::any_object::run_some()`), which will process a number of messages, and if there are any remaining messages, re-activate the AO to re-schedule its execution.

In this way, `active::pool` is able to process many AOs in a small number of threads.

== Queueing implementation ==
The queue determines which message to service next, and provides a container for storing messages. The provide queues are:

  * `active::queueing::shared`  (default) - uses a single linked list to store all messages for an object
  * `active::queueing::separate` - an alternative implementation.
  * `active::queuing::direct_call` - Process message immediately in calling thread without a mutex.
  * `active::queueing::mutexed_call` - Process message immediately in calling thread, but mutexes the object. Reentrant calls will deadlock.
  * `active::queueing::try_lock` - Attempt to lock the object but throw an exception on failure.
  * `active::queuing::steal<>` - Adapt another queueing strategy by processing the message in the calling thread when the object is idle.

== Sharing implementations ==
Sharing information controls the pointer type in the message queue. The main use case for this is to use shared pointers (`std::shared_ptr<>`) in the message queue, which guarantees that AOs with messages cannot be destroyed.

  * `active::sharing::enabled<>` - use `std::shared_ptr`.
  * `active::sharing::disabled` - use C-style pointers.

= Active object guarantees =

An implementation may do whatever it likes provided that the following characteristics, or Guarantees are met:

G1. Scalability: work with a large number of objects. For example, limiting AOs to the number of OS threads could be too limiting for some applications.

G2. Non-overlapping: Objects process at most one message at a time.
 a. Non-reentrant: Messages which send messages back to the same object (perhaps indirectly) are processed after the current message is processed.

 b. Across different threads: An AO does not execute in two or more threads simultaneously.
G3. Non-blocking: Sending a message to an AO does not cause a large delay.
 a. Non CPU-wait guarantee

 b. Non-deadlock guarantee

 c. Non IO-block guarantee

G4. Recursion - Messages can be posted recursively to a great depth.

G5. Concurrency: different AOs can run on different hardware processing units concurrently. 

G6. Delivery: Messages sent to the object are guaranteed to be delivered.

G7. Safety: 

  a. safe from concurrent access,  

 b. safe from dangling pointers 

 c. safe from memory leaks.

G8. Ordering: Messages are processed in the order they are received. Variations could include processing messages based on priority.

G9. Fairness: Each AO gets a fair slice of CPU time, and most importantly is not starved.

I just made these up - maybe you could think of some more.

|| Objects || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || G10 ||
|| `object`   || y || y  || y  || y  || y  || y(1) || yny|| y  || y(2) || 
|| `shared`   || y || y  || y  || y  || y  || y(1) || yyy|| y  || y(2) || 
|| `fast`       ||
|| `direct` ||
|| `mutexed` ||
|| `try_lock`||
|| `thread` ||
|| `shared_thread` ||

|| Scheduling || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || G10 ||
|| `none` ||
|| `thread_pool` ||
|| `own_thread` ||

|| Queueing || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || G10 ||
|| `shared` ||
|| `separate` ||
|| `steal<>` || 
|| `direct_call` ||
|| `mutexed_call` ||
|| `try_mutex` ||

|| Sharing || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || G10 ||
|| `enabled<>` ||
|| `disabled` ||

Notes:

  #. Failure would only occur on running out of memory (`std::bad_alloc`).
  #. If there are too many blocking objects, and the pool of threads is too small, then there could be a bottleneck. Also objects are allocated a slot per message, not per unit time, so greedy objects would be executed more.

= Performance =

|| Typename || Schedule || Queueing || Sharing || Million messages per second ||
|| `active::direct_call` || `none` || `direct_call` || `disabled` || 26 ||
|| || `none` || `direct_call` || `enabled<>` || 26 ||
|| `active::fast` || `thread_pool` || `steal<shared>` || disabled || 8.6 ||
|| || `thread_pool` || `steal<shared>` || enabled || 7.7 ||
|| || thread_pool || `steal<separate>` || disabled || 8.5 ||
|| || thread_pool || `steal<separate>` || enabled || 8.6 ||
|| `active::object` || `thread_pool` || `shared` || `disabled` || 3.2 ||
|| `active::shared<>` || `thread_pool` || `shared` || `enabled<>` || 2.8 ||
|| || `thread_pool` || `separate` || `disabled` || 3.6 ||
|| || `thread_pool` || `separate` || `enabled<>` || 3.1 ||
|| `active::thread` || `own_thread` || `shared` || `disabled` || 0.125 ||
|| `active::shared_thread<>` || `own_thread` || `shared` || `enabled<>` || 0.133 ||
