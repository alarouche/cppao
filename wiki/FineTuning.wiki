#summary Getting the most out of cppao.

= Summary =
The basic object types (`active::object` and `active::shared<>`) are the default types which should offer reasonable results.  Cppao offers other object types with slightly different characteristics.

Cppao actually implements a policy framework, where different aspects can be combined. It is possible to implement new strategies (e.g. to prioritise messages, or to limit the message queue size) in an extensible manner.

= Object types =

* Beware: Here be dragons *

The basic object type is `active::object`, with `active::shared<>` for use with `std::shared_ptr<>`. These are designed to offer the best and safest experience. The implementation details are deliberately hidden because it allows the underlying implementation to change and innovate.

It is possible to select different implementations if you want to try to improve performance, perhaps at the expense of a little safety. Like much multi-threaded code, performance is not always predictable.

See [http://code.google.com/p/cppao/source/browse/trunk/samples/object_types.cpp object_types.cpp] for examples of the different object types.

Available object types:

|| Type name || Description || Use case ||
|| `active::object` || The basic AO offering a good default. It is not completely safe because the standard C++ scoping rules apply, and attempting to destroy the object during message processing will result in defined behaviour. || Default use. ||
|| `active::fast` || Implements "work stealing" strategy so that a message call can actually execute the message handler if the object is currently idle. This object is safe, but the caller can become blocked. If you have an insane number of active objects, it's possible to run out of stack space. || When processing is quick and concurrent execution is not necessary. ||
|| `active::shared<>` || Same as `active::object` but for use in smart pointers, and is guaranteed to not be destroyed during message processing. || When using `std::shared_ptr<>` ||
|| `active::thread` || Dedicates an OS thread to this object. This object is safe, but can fail due to OS limits on the number of threads. || When you are sure you need a dedicated thread. ||
|| `active::shared_thread<>` || Dedicates an OS thread to this object, for used in shared objects. || When you are sure you need a dedicated thread and using `std::shared_ptr` ||

== Fake AOs ==

The following classes are very similar to normal AOs, and are in many cases much faster. Their main use is for interfacing with other AOs (e.g. to implement callbacks), or for experimenting with a blend of object types for optimum performance.

|| Type name || Description || Use case ||
|| `active::synchronous` || A very simple mutexed object, which does not use queueing. Could be reentrant (so not true AO semantics). || For lightweight thread-safe processing. ||
|| `active::direct` || Almost the same as a standard function call, with no locking mechanisms at all. Beware of stack overflows as well. || Lightweight single-threaded processing. When you have your own locking mechanism. ||

= Policy framework =

Cppao actually implements a policy framework which allows different types of AO to be created, and even supports the creation of new implementation strategies by the developer.

The underlying AO is implemented by the `active::object_impl<>` template which needs to be supplied with three different strategy-types (using a Strategy pattern supplied as template parameters). The built-in AO types are just typedefs of `active::object_impl<>`.

{{{
namespace active
{
    template<typename Schedule, typename Queueing, typename Sharing>
    struct object_impl;
}
}}}

== Schedule implementations ==
The schedule determines which AO  is next run.  The provided schedulers are:

  * `active::schedule::thread_pool` (default) - schedules the AO using a thread-pool. This is ideal if the number of AOs is large.
  * `active::schedule::own_thread` - uses a dedicated OS thread to execute the AO. Good if you need pre-emptive multitasking and the number of AOs is limited (<1000 say).
  * `active::schedule::none` - used for those queueing implementations which don't need their own scheduler.

An AO becomes "activated" when it receives its first message. The `thread_pool` scheduler passes the AO to `active::pool`, which puts the AO onto a FIFO of activated objects. A thread (or pool of threads) can then pop the next AO from the queue, and run the AO (using `active::any_object::run_some()`), which will process a number of messages, and if there are any remaining messages, re-activate the AO to re-schedule its execution.

In this way, `active::pool` is able to process many AOs in a small number of threads.

== Queueing implementation ==
The queue determines which message to service next, and provides a container for storing messages. The provide queues are:

  * `active::queueing::shared`  (default) - uses a single linked list to store all messages for an object
  * `active::queueing::separate` - an alternative implementation.
  * `active::queuing::direct_call` - Process message immediately in calling thread without a mutex.
  * `active::queueing::mutexed_call` - Process message immediately in calling thread, but mutexes the object. Reentrant calls will deadlock.
  * `active::queueing::try_lock` - Attempt to lock the object but throw an exception on failure.
  * `active::queuing::steal<>` - Adapt another queueing strategy by processing the message in the calling thread when the object is idle.

== Sharing implementations ==
Sharing information controls the pointer type in the message queue. The main use case for this is to use shared pointers (`std::shared_ptr<>`) in the message queue, which guarantees that AOs with messages cannot be destroyed.

  * `active::sharing::enabled<>` - use `std::shared_ptr`.
  * `active::sharing::disabled` - use C-style pointers.

= Active object guarantees =

An implementation may do whatever it likes provided that the following characteristics, or _guarantees_ are met:

  #. *Scalable*: work with a large number of objects. For example, limiting AOs to the number of OS threads could be too limiting for some applications.
  #. *Non-overlapping*: Objects process at most one message at a time.
    * *Non-reentrant*: Messages which send messages back to the same object (perhaps indirectly) are processed after the current message is processed.
    * *Across different threads*: An AO does not execute in two or more threads simultaneously.
  #. *Non-blocking*: Sending a message to an AO does not cause a large delay.
    * Non CPU-wait guarantee
    * Non-deadlock guarantee
    * Non IO-block guarantee
  #. *Recursive*: Messages can be posted recursively to a great depth.
  #. *Concurrent*: different AOs can run on different hardware processing units concurrently. 
  #. *Deliverable*: Messages sent to the object are guaranteed to be delivered.
  #. *Safe*: 
    * safe from concurrent access,  
    * safe from dangling pointers, 
    * safe from memory leaks.
  #. *Ordered*: Messages are processed in the order they are received. Variations could include processing messages based on priority.
  #. *Fair*: Each AO gets a fair slice of CPU time, and most importantly is not starved.

I just made these up - maybe you could think of some more.

|| Object type || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || 
|| `active::object`   || y || y  || y  || y  || y  || y^1^ || yny|| y  || y^2^ || 
|| `active::shared<>`   || y || y  || y  || y  || y  || y^1^ || yyy|| y  || y^2^ || 
|| `active::fast`       || y || y || n || y || y^3^ || y || yny || y || y ||
|| `active::direct` || y || nn || n || n || n || y^1^ || nnn || y || n || 
|| `active::mutexed` || y || y || n || n || n || n || ynn || y || n ||
|| `active::try_lock`|| y || y || n || n || n || n || ynn || y || n ||
|| `active::thread` || n || y || y || y || y || y || ynn || y || y ||
|| `active::shared_thread<>` || n || y || y || y || y || y || yyy || y || y ||

Notes:

  #. Failure would only occur on running out of memory (`std::bad_alloc`).
  #. If there are too many blocking objects, and the pool of threads is too small, then there could be a bottleneck. Also objects are allocated a slot per message, not per unit time, so greedy objects would be executed more.
  #. The issue is that if you end up doing all of your work in the calling thread, then there is no opportunity for concurrency.

= Performance =

These figures are generated by the thread-ring test [http://code.google.com/p/cppao/source/browse/trunk/samples/bench.cpp bench.cpp] - YMMV.

|| Typename || Schedule || Queueing || Sharing || Million messages per second ||
|| `active::direct` || `none` || `direct_call` || `disabled` || 26 ||
|| || `none` || `direct_call` || `enabled<>` || 26 ||
|| `active::fast` || `thread_pool` || `steal<shared>` || `disabled` || 8.6 ||
|| || `thread_pool` || `steal<shared>` || `enabled` || 7.7 ||
|| || `thread_pool` || `steal<separate>` || `disabled` || 8.5 ||
|| || `thread_pool` || `steal<separate>` || `enabled` || 8.6 ||
|| `active::object` || `thread_pool` || `shared` || `disabled` || 3.2 ||
|| `active::shared<>` || `thread_pool` || `shared` || `enabled<>` || 2.8 ||
|| || `thread_pool` || `separate` || `disabled` || 3.6 ||
|| || `thread_pool` || `separate` || `enabled<>` || 3.1 ||
|| `active::thread` || `own_thread` || `shared` || `disabled` || 0.125 ||
|| `active::shared_thread<>` || `own_thread` || `shared` || `enabled<>` || 0.133 ||