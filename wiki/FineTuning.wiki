#summary Getting the most out of cppao.

= Summary =
This page goes into quite a lot of detail about design considerations and the various object types, and is best avoided on first reading.

The basic object types (`active::object` and `active::shared<>`) are the default types which should offer reasonable results.  Cppao offers other object types with  different characteristics.

Cppao actually implements a policy framework, where different aspects can be combined. It is possible to implement new strategies (e.g. to prioritise messages, or to limit the message queue size) in an extensible manner.

= Object types =

* Beware: Here be dragons *

The basic object type is `active::object`, with `active::shared<>` for use with `std::shared_ptr<>`. These are designed to offer the best and safest experience. The implementation details are deliberately hidden because it allows the underlying implementation to change and innovate.

It is possible to select different implementations if you want to try to improve performance, perhaps at the expense of a little safety. Like much multi-threaded code, performance is not always predictable.

See [http://code.google.com/p/cppao/source/browse/trunk/samples/object_types.cpp object_types.cpp] for examples of the different object types.

Available object types:

|| Type name || Description || Use case ||
|| `active::object` || The basic AO offering a good default. It is not completely safe because the standard C++ object lifetime rules apply. Uses a faster scheduler than OS threads (see Performance later) || Default use. ||
|| `active::fast` || Implements "work stealing" strategy so that a message call can actually execute the message handler if the object is currently idle. This object is safe, but the caller can become blocked. If you have an insane number of active objects, it's possible to run out of stack space. || When processing is quick and concurrent execution is not necessary. ||
|| `active::shared<>` || Same as `active::object` but for use in smart pointers, and is guaranteed to not be destroyed during message processing. || When using `std::shared_ptr<>` ||
|| `active::thread` || Dedicates an OS thread to this object. This object is safe, but can fail due to OS limits on the number of threads. || When you are sure you need a dedicated thread. ||
|| `active::shared_thread<>` || Dedicates an OS thread to this object, for used in shared objects. || When you are sure you need a dedicated thread and using `std::shared_ptr` ||

== Fake AOs ==

The following classes are very similar to normal AOs, and are in many cases much faster. Their main use is for interfacing with other AOs (e.g. to implement callbacks), for simple connecting objects, or for experimenting with a blend of object types for optimum performance. Beware stack overflows.

|| Type name || Description || Use case ||
|| `active::synchronous` || A very simple mutexed object, which does not use queueing. Could be reentrant (so not true AO semantics). || For lightweight thread-safe processing. ||
|| `active::direct` || Almost the same as a standard function call, with no locking mechanisms at all. || Lightweight single-threaded processing. When the object is already threadsafe or has no mutable state. ||

= Policy framework =

Cppao actually implements a policy framework which allows different types of AO to be created, and even supports the creation of new implementation strategies by the developer.

The underlying AO is implemented by the `active::object_impl<>` template which needs to be supplied with three different strategy-types (using a Strategy pattern supplied as template parameters). The built-in AO types are just typedefs of `active::object_impl<>`.

{{{
namespace active
{
    template<typename Schedule, typename Queueing, typename Sharing>
    struct object_impl;
}
}}}

== Schedule implementations ==
The schedule determines which AO  is next run.  The provided schedulers are:

  * `active::schedule::thread_pool` (default) - schedules the AO using a thread-pool. This is ideal if the number of AOs is large.
  * `active::schedule::own_thread` - uses a dedicated OS thread to execute the AO. Good if you need pre-emptive multitasking and the number of AOs is limited (<1000 say).
  * `active::schedule::none` - used for those queueing implementations which don't need their own scheduler.

An AO becomes "activated" when it receives its first message. The `thread_pool` scheduler passes the AO to `active::pool`, which puts the AO onto a FIFO of activated objects. A thread (or pool of threads) can then pop the next AO from the queue, and run the AO (using `active::any_object::run_some()`), which will process a number of messages, and if there are any remaining messages, re-activate the AO to re-schedule its execution.

In this way, `active::pool` is able to process many AOs in a small number of threads.

== Queueing implementation ==
The queue determines which message to service next, and provides a container for storing messages. The provide queues are:

  * `active::queueing::shared`  (default) - uses a single linked list to store all messages for an object
  * `active::queueing::separate` - an alternative implementation.
  * `active::queuing::direct_call` - Process message immediately in calling thread without a mutex.
  * `active::queueing::mutexed_call` - Process message immediately in calling thread, but mutexes the object with a recursive mutex.
  * `active::queueing::steal<>` - Adapt another queueing strategy by processing the message in the calling thread when the object is idle.

== Sharing implementations ==
Sharing information controls the pointer type in the message queue. The main use case for this is to use shared pointers (`std::shared_ptr<>`) in the message queue, which guarantees that AOs with messages cannot be destroyed.

  * `active::sharing::enabled<>` - use `std::shared_ptr`.
  * `active::sharing::disabled` - use C-style pointers.

= Active object guarantees =

The basic _semantics_ of AOs are that they execute _as if_ they are running in an independent thread, and that they sequentially process their messages. That does not require an OS thread of course, and an implementation may do whatever it likes provided that the following characteristics, or _guarantees_ are met:

  #. *Scalable*: Work with a large number of objects. For example, limiting AOs to the number of OS threads could be too limiting for some applications.
  #. *Non-overlapping*: Objects process at most one message at a time.
    * *Non-reentrant*: Messages which send messages back to the same object (perhaps indirectly) are processed after the current message is processed.
    * *Non-concurrent*: An AO does not execute in two or more threads simultaneously.
  #. *Non-blocking*: Sending a message to an AO does not cause a large delay.
    * Non CPU-wait guarantee
    * Non-deadlock guarantee
    * Non IO-block guarantee
  #. *Recursive*: Messages can be posted recursively to a great depth.
  #. *Concurrent*: Different AOs can run on different hardware processing units concurrently. 
  #. *Deliverable*: Messages sent to the object are guaranteed to be delivered.
  #. *Safe*: 
    * safe from concurrent access,  
    * safe from dangling pointers, 
    * safe from memory leaks.
  #. *Ordered*: Messages are processed in the order they are received. Variations could include processing messages based on priority.
  #. *Fair*: Each AO gets a fair slice of CPU time, and most importantly is not starved.

I just made these up - maybe you could think of some more.

|| Object type               || G1 || G2 || G3 || G4 || G5 || G6 || G7 || G8 || G9 || 
|| `active::object`          || y  || y,y|| y  || y  || y  || y^1^ || y,n,y || y  || y^2^ || 
|| `active::shared<>`        || y  || y,y|| y  || y  || y  || y^1^ || y,y,n^5^ || y || y^2^ || 
|| `active::fast`            || y  || y,y|| n || y || y^3^ || y || y,n,y || y || y^2^ ||
|| `active::direct`          || y  || n,n|| n || n || n || y^1^ || n,n,y || y || n || 
|| `active::synchronous`     || y  || n,y|| n || n || n || y^4^ || y,n,n^5^ || y || n ||
|| `active::thread`          || n  || y,y|| y || y || y || y || y,n,y || y || y ||
|| `active::shared_thread<>` || n  || y,y|| y || y || y || y || y,y,n^5^ || y || y ||

Notes:

  #. Failure would only occur on running out of memory (`std::bad_alloc`).
  #. Objects are visited in a round-robin, however objects can hog the CPU or worse, wait on IO, which would starve other objects.
  #. The issue is that if you end up doing all of your work in the calling thread, then there is no opportunity for concurrency.
  #. Unless you run out of stack space.
  #. You need `std::weak_ptr<>` to handle cycles.

= Performance =

These figures are generated by the thread-ring test [http://code.google.com/p/cppao/source/browse/trunk/samples/bench.cpp bench.cpp] - YMMV.

|| Typename || Schedule || Queueing || Sharing || Million messages per second ||
|| `active::direct` || `none` || `direct_call` || `disabled` || 25 ||
|| `active::synchronous` || `none` || `mutexed_call` || `disabled` || 19 ||
|| `active::fast` || `thread_pool` || `steal<shared>` || `disabled` || 19 ||
|| || `thread_pool` || `steal<shared>` || `enabled<>` || 19 ||
|| || `thread_pool` || `steal<separate>` || `disabled` || 16 ||
|| || `thread_pool` || `steal<separate>` || `enabled` || 17 ||
|| `active::object` || `thread_pool` || `shared` || `disabled` || 5.1 ||
|| `active::shared<>` || `thread_pool` || `shared` || `enabled<>` || 4.0 ||
|| || `thread_pool` || `separate` || `disabled` || 7.1 ||
|| || `thread_pool` || `separate` || `enabled<>` || 4.7 ||
|| `active::thread` || `own_thread` || `shared` || `disabled` || 0.19 ||
|| `active::shared_thread<>` || `own_thread` || `shared` || `enabled<>` || 0.19 ||

== Comment ==
Using the OS to schedule AOs is of course a reasonable first approach, but cppao implements its own scheduler which is much more scalable and lightweight, and the performance results confirm this.

Hopefully your application is written such that most of the CPU is expended on doing your computation than the overhead of messaging. Some design consideration is needed to get the "right amount" of messaging, so that the overheads of messaging do not outweigh the benefits of concurrency.

With cppao, you can fairly easily reconfigure your active objects using the different AO types.