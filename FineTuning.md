# Active object guarantees #

The basic _semantics_ of AOs are that they execute _as if_ they are running in an independent thread, and that they sequentially process their messages. That does not require an OS thread of course, and an implementation may do whatever it likes provided that the following characteristics, or _guarantees_ are met:

  1. **Scalable**: Work with a large number of objects. For example, limiting AOs to the number of OS threads could be too limiting for some applications.
  1. **Non-overlapping**: Objects process at most one message at a time.
    * **Non-reentrant**: Messages which send messages back to the same object (perhaps indirectly) are processed after the current message is processed.
    * **Non-concurrent**: An AO does not execute in two or more threads simultaneously.
  1. **Non-blocking**: Sending a message to an AO does not cause a large delay.
    * Non CPU-wait guarantee
    * Non-deadlock guarantee
    * Non IO-block guarantee
  1. **Recursive**: Messages can be posted recursively to a great depth.
  1. **Concurrent**: Different AOs can run on different hardware processing units concurrently.
  1. **Deliverable**: Messages sent to the object are guaranteed to be delivered.
  1. **Safe**:
    * safe from concurrent access,
    * safe from dangling pointers,
    * safe from memory leaks.
  1. **Ordered**: Messages are processed in the order they are received. Variations could include processing messages based on priority.
  1. **Fair**: Each AO gets a fair slice of CPU time, and most importantly is not starved.

I just made these up - maybe you could think of some more.

| Object type               | G1 | G2 | G3 | G4 | G5 | G6 | G7 | G8 | G9 |
|:--------------------------|:---|:---|:---|:---|:---|:---|:---|:---|:---|
| `active::basic`           | y  | y,y| y  | y  | y  | y<sup>1</sup> | y,n,y | y  | y<sup>2</sup> |
| `active::shared<>`        | y  | y,y| y  | y  | y  | y<sup>1</sup> | y,y,n<sup>5</sup> | y  | y<sup>2</sup> |
| `active::fast`            | y  | y,y| n  | y  | y<sup>3</sup> | y  | y,n,y | y  | y<sup>2</sup> |
| `active::direct`          | y  | n,n| n  | n  | n  | y<sup>1</sup> | n,n,y | y  | n  |
| `active::synchronous`     | y  | n,y| n  | n  | n  | y<sup>4</sup> | y,n,n<sup>5</sup> | y  | n  |
| `active::thread`          | n  | y,y| y  | y  | y  | y  | y,n,y | y  | y  |
| `active::advanced`        | y  | y,y| y  | y  | y  | y<sup>1</sup> | y,n,y | y<sup>6</sup> | y<sup>2</sup> |

Notes:

  1. Failure would only occur on running out of memory (`std::bad_alloc`).
  1. Objects are visited in a round-robin, however objects can hog the CPU or worse, wait on IO, which would starve other objects.
  1. The issue is that if you end up doing all of your work in the calling thread, then there is no opportunity for concurrency.
  1. Unless you run out of stack space.
  1. You need `std::weak_ptr<>` to handle cycles.
  1. Messages of the same priority are correctly ordered.

# Performance #

These figures are generated by the thread-ring test [bench.cpp](http://code.google.com/p/cppao/source/browse/trunk/samples/bench.cpp) - YMMV.

| Test | Active object type | Million messages per second |
|:-----|:-------------------|:----------------------------|
| 1    | `active::direct`   | 102                         |
| 2    | `active::shared<T,active::direct>` | 88                          |
| 3    | `active::synchronous` | 44                          |
| 4    | `active::shared<T,active::synchronous>` | 44                          |
| 5    | `active::fast`     | 18                          |
| 6    | `active::shared<T,active::fast>` | 18                          |
| 7    | `active::object`   | 5.2                         |
| 8    | `active::shared<T>`  | 4.1                         |
| 9    | `active::advanced` | 4.9                         |
| 10   | `active::shared<T,active::advanced>` | 3.9                         |
| 11   | `active::thread`   | 0.37                        |
| 12   | `active::shared<T,active::thread>` | 0.34                        |

## Comment ##
Using the OS to schedule AOs is of course a reasonable first approach, but cppao implements its own scheduler which is much more scalable and lightweight, and the performance results confirm this.

Hopefully your application is written such that most of the CPU is expended on doing your computation than the overhead of messaging. Some design consideration is needed to get the "right amount" of messaging, so that the overheads of messaging do not outweigh the benefits of concurrency.

With cppao, you can fairly easily reconfigure your active objects using the different AO types.